---
layout: post
title: Trust Region Policy Optimization
feature-img: "assets/img/tree.png"
tags: [强化学习, 论文笔记]
---
Natural Policy Gradient, Trust Region Policy Optimization

[Trust region policy optimization.](http://proceedings.mlr.press/v37/schulman15.pdf) Schulman, L., Moritz, Jordan, Abbeel. 2015  
[A Natural Policy Gradient](https://proceedings.neurips.cc/paper/2001/file/4b86abe48d358ecf194c56c69108433e-Paper.pdf) Kakade, 2001  

参考：[techblog: Natural Gradient Descent](https://agustinus.kristia.de/techblog/2018/03/14/natural-gradient/)  
&emsp;&emsp;&emsp;[知乎：自然策略梯度](https://zhuanlan.zhihu.com/p/546885304)

## 1 Natural Policy Gradient

&emsp;&emsp;标准梯度下降$$\pmb{\theta}_{k+1} = \pmb{\theta}_k - \alpha_k \nabla J(\pmb{\theta}_k)$$无法保证目标函数$$J(\pmb{\theta})$$按最大速度下降，因为标准梯度下降是非协变的 (non-covariant)，$$\pmb{\theta}$$的参数空间不是欧几里得的。一般来说参数空间具有黎曼度量结构，如神经网络。在这些情况下，普通梯度不能给出目标函数最陡峭的方向，所以[A Natural Policy Gradient](https://proceedings.neurips.cc/paper/2001/file/4b86abe48d358ecf194c56c69108433e-Paper.pdf)一文提出了能够实现参数空间最速下降的自然梯度。

### 1.1 Riemannian Space

&emsp;&emsp;欧式空间具有标准正交坐标系，其微分向量的平方长度为$$\vert d w \vert^2 = \sum_{i=1}^n (d w_i)^2$$。而黎曼空间是一种矢量空间，不具有正交坐标系，其微分向量的平方长度由正定二次型决定

$$\vert d \boldsymbol{w} \vert^2 = \sum_{i,j} \mathbf{G} dw_i dw_j$$

其中$$\mathbf{G}$$是一个$$n * n$$的对称矩阵。当$$\mathbf{G}$$为单位对角阵$$\mathbf{I}$$时，黎曼空间退化为欧氏空间。

&emsp;&emsp;黎曼空间中最速下降的梯度为

$$\tilde{\nabla} L(\boldsymbol{w}) = \mathbf{G}^{-1}(\boldsymbol{w}) \nabla L(\boldsymbol{w})$$

其中$$G$$可以用Fisher信息矩阵$$F$$替换。【[证明](https://zhuanlan.zhihu.com/p/546885304)】

&emsp;&emsp;似然函数$$L(\theta \mid x) = f_{\theta}(x) = f(\theta \mid x)$$ ($$L(\theta \mid x) = p(X = x; \theta) = p_{\theta}(x)$$)，对数似然函数为$$s(\theta) = \nabla_{\theta} \log p(x \mid \theta)$$，其Fisher矩阵为

$$  \mathbf{F} 
 =  \mathbb{E}_{p(x \mid \theta)}[\nabla \log p(x \mid \theta) \nabla \log p(x \mid \theta)^T]
$$

### 1.2 A Natural Gradient

&emsp;&emsp;一个有限MDP可以描述为$$(S, s_0, A, R, P)$$，agent的决策根据随机策略$$\pi (a; s)$$。假设每个策略$$\pi$$有静态分布$$\rho^{\pi}$$，那么average reward（或undiscounted reward）为

$$\eta(\pi) \equiv \sum_{s, a} \rho^\pi(s) \pi(a; s) R(s, a)$$

state-action值为

$$  Q^\pi(s, a) \equiv 
    E_\pi  \left\{
                \sum_{t=0}^{\infty} R\left(s_t, a_t\right) - \eta(\pi)\mid s_0=s, a_0=a
           \right\}
$$

value funtion为

$$J^{\pi} \equiv E_{\pi(a';s)} \{ Q^\pi(s, a') \}$$

&emsp;&emsp;考虑更复杂的情况，目标是找到一个能最大化平均奖励的策略且该策略是平滑参数化的，$$\tilde{\Pi} = \left\{ \pi_\theta: \theta \in \Re^m \right\}$$，其中$$\pi_\theta$$表示$$\pi(a; s, \theta)$$。

&emsp;&emsp;那么平均奖励的精确梯度为

$$\nabla \eta(\theta) = \sum_{s, a} \rho^\pi(s) \nabla \pi(a; s, \theta) Q^\pi(s, a)$$

其中，我们用$$\eta(\theta)$$来简写$$\eta(\pi_{\theta})$$。

&emsp;&emsp;最速下降方向向量为$$d \theta$$，其平方长度为$$\vert d \theta \vert^2 = \sum_{i,j} G_{i,j} d \theta_i d \theta_j = d \theta^T G(\theta) d \theta$$，所以最速下降的方向为$$G^{-1} \nabla \eta(\theta)$$。对于每个状态$$s$$都有一个对应的概率流形，分布$$\pi(a; s, \theta)$$是坐标系为$$\theta$$的流形上的一点。分布$$\pi(a; s, \theta)$$的Fisher信息矩阵为

$$      F_s(\theta) 
\equiv  \mathbb{E}_{\pi(a; s, \theta)} \left[
            \frac{\partial \log \pi(a ; s, \theta)}{\partial \theta_i} \frac{\partial \log \pi(a ; s, \theta)}{\partial \theta_j}
        \right]
$$

<b><font color="#3399ff">Fisher信息矩阵在一定程度上是概率分布参数空间上的不变度量</font></b>，因为用它定义的距离与坐标系的选择（即参数化）如何无关。

&emsp;&emsp;由于平均奖励是在这些分布的集合上定义的，我们为度量做出的直接选择是

$$\color{green}{
    F(\theta) \equiv \mathbb{E}_{\rho^{\pi}}[F_s(\theta)]
}$$

其中期望与$$\pi_{\theta}$$的静态分布有关。值得注意的是，虽然每个$$F_s$$独立于MDP转移模型的参数，但是<b><font color="#3399ff">静态分布的权重引入了对这些参数的依赖性</font></b>。

&emsp;&emsp;最终得到的<b><font color="#00B050">自然梯度</font></b>为

$$\color{green}{
    \widetilde{\nabla} \eta(\theta) \equiv F(\theta)^{-1} \nabla \eta(\theta)
}$$

## 2 Trust Region Policy Optimization

&emsp;&emsp;一种优化控制策略的方法，可保证单调改进。通过对理论上合理的方案进行一些近似，开发了一种实用的算法，称为信任区域策略优化 (TRPO)。该方法对优化更大的非线性策略有效，比如神经网络。尽管其近似偏离了理论，但是TRPO倾向于给出单调改进，且几乎不需要调整超参数。

### 2.1 Preliminaries

&emsp;&emsp;无限MDP表示为$$(\mathcal{S}, \mathcal{A}, P, c, \rho_0, \gamma)$$，其中$$P: \mathcal{S} \times \mathcal{A} \times \mathcal{S}$$为转移概率分布，$$c$$为cost，$$\rho_0$$为初态$$s_0$$的概率分布。随机策略为$$\pi : \mathcal{S} \times \mathcal{A}$$。

&emsp;&emsp;期望折扣cost为

$$\begin{aligned}
&   \eta(\pi) = 
        \mathbb{E}_{s_0, a_0, \ldots} \left[ \sum_{t=0}^{\infty} \gamma^t c (s_t) \right],  \\
&   \text { where } 
    s_0 \sim \rho_0\left(s_0\right), 
    a_t \sim \pi\left(a_t \mid s_t\right), 
    s_{t+1} \sim P\left(s_{t+1} \mid s_t, a_t\right)
\end{aligned}
    
$$

&emsp;&emsp;state-action value function $$Q_\pi$$，value function $$V_\pi$$和advantage function $$A_\pi$$分别为

$$\begin{aligned}
&   Q_\pi (s_t, a_t) = \mathbb{E}_{s_{t+1}, a_{t+1}, \ldots} 
                       \left[\sum_{l=0}^{\infty} \gamma^l c (s_{t+l}) \right], \\
&   V_\pi t(s_t) =  \mathbb{E}_{a_t, s_{t+1}, \ldots} 
                    \left[ \sum_{l=0}^{\infty} \gamma^l c (s_{t+l}) \right], \\
&   A_\pi (s, a) =  Q_\pi(s, a)-V_\pi(s),   \\
&   \text { where } a_t \sim \pi (a_t \mid s_t), 
                    s_{t+1} \sim P\left(s_{t+1} \mid s_t, a_t\right) 
    \text { for } t \geq 0 .
\end{aligned}$$

&emsp;&emsp;策略$$\tilde{\pi}$$的期望折扣cost $$\eta$$，用$$\pi$$的adavantage函数表示为

$$\begin{aligned}
&   \eta(\tilde{\pi}) = \eta(\pi) + \mathbb{E}_{s_0, a_0, s_1, a_1, \ldots}
                        \left[ \sum_{t=0}^{\infty} \gamma^t A_\pi (s_t, a_t) \right],   \\
&   \text { where } 
    s_0 \sim \rho_0 (s_0), 
    a_t \sim \tilde{\pi} (a_t \mid s_t), 
    s_{t+1} \sim P (s_{t+1} \mid s_t, a_t)
\end{aligned} \tag{1}$$

&emsp;&emsp;令$$\rho_\pi$$为(非归一化的)折扣访问频率

$$  \rho_\pi(s)
 =  \left( P(s_0=s)+\gamma P(s_1=s)+\gamma^2 P(s_2=s)+\ldots \right)
$$

把(1)改写为按状态求和，得到

$$  \eta(\tilde{\pi})
 =  \eta(\pi) + \sum_s \rho_{\tilde{\pi}}(s) \sum_a \tilde{\pi}(a \mid s) A_\pi(s, a)
\tag{2}$$

表明每次策略更新$$\pi \rightarrow \tilde{\pi}$$时，对于所有状态$$s$$都有非正的期望advantage，即$$\sum_a \tilde{\pi}(a \mid s) A_\pi(s, a) \le 0$$必能让$$\eta$$变小。当用确定性策略$$\tilde{\pi}(s) = \operatorname{arg} \min_a A_{\pi}(s, a)$$更新时，当至少有一个state-action的$$A$$为负且有非零访问概率时，才会改进策略。但是，由于估计和近似误差的存在，无法保证$$A \le 0$$。并且$$\rho_{\tilde{\pi}}(s)$$与$$\tilde{\pi}$$的关系复杂，所以(2)无法直接优化。

&emsp;&emsp;对$$\eta(\tilde{\pi})$$作局部近似，用$$\rho_\pi$$代替$$\rho_{\tilde{\pi}}$$，得到

$$\color{green}{\boxed{
    L_{\pi}(\tilde{\pi}) 
  = \eta(\pi) + \sum_s \rho_{\pi}(s) \sum_a \tilde{\pi}(a \mid s) A_\pi(s, a)
}}\tag{3}$$

对于参数化策略$$\pi_{\theta}$$，若$$\pi_{\theta}(a \mid s)$$关于$$\theta$$可微，那么$$L_{\pi}$$符合$$\eta$$的一阶等效，即二者关于$$\theta$$的梯度相等。

**How big a step?**  
— 保守策略迭代 (conservative policy iteration)，给出$$\eta$$改进的明确下界。

&emsp;&emsp;当前策略为$$\pi_{\text{old}}$$，假设可解$$\pi' = \operatorname{arg} \min_{\pi'} L_{\pi_{\text{old}}} (\pi')$$。新策略采用以下混合策略：

$$  \pi_{\text {new }}(a \mid s)
  = (1-\alpha) \pi_{\text {old }}(a \mid s)+\alpha \pi^{\prime}(a \mid s)
\tag{5}$$

Kakade and Langford为该更新证明了以下结果：

$$  \eta (\pi_{\text {new }}) 
\le L_{\pi_{\text{old}}}(\pi_{\text {new }})
    + \frac{2 \epsilon \gamma}{(1-\gamma(1-\alpha))(1-\gamma)} \alpha^2
\tag{6}$$

其中，$$\epsilon$$是$$\pi'$$关于$$\pi$$的最大advantage（正或负）

$$  \epsilon
  = \max _s \left|\mathbb{E}_{a \sim \pi^{\prime}(a \mid s)}\left[A_\pi(s, a)\right] \right|
$$

由于$$\alpha, \gamma \in [0, 1]$$，(6)意味着以下更简单的界限

$$  \eta (\pi_{\text {new }}) 
\le L_{\pi_{\text{old}}}(\pi_{\text {new }})
    + \frac{2 \epsilon \gamma}{(1-\gamma)^2} \alpha^2
\tag{8}$$

此界限仅在$$\alpha \ll 1$$时略弱，Kakade & Langford (2002)的保守策略迭代方法通常就是这种情况。此界限只适用于(5)生成的混合策略，该策略再实践中笨拙且受限，所以我们期望能有个**实用的策略更新方案，能够适用于所以一般随机策略类型**。

### 2.2 Monotonic Improvement Guarantee for General Stochastic Policies

&emsp;&emsp;主要理论成果：通过用$$\pi$$和之间的$$\tilde{\pi}$$的距离代替$$\alpha$$，将由(6)约束的<b><font color="#3399ff">策略改进推广到一般随机策略</font></b>，而非仅仅是混合策略。 

&emsp;&emsp;距离度量采用总二分散度 (<b><font color="#00B050">total variation divergence</font></b>)。离散概率分布$$p, q$$之间的总二分散度为$$D_{\text{TV}} (p \| q) = \frac{1}{2} \sum_i \vert p_i - q_i \vert$$，定义

$$  D_{\text{TV}}^{\max}(\pi, \tilde{\pi})
  = \max_s D_{\text{TV}} \big( \pi(\cdot \mid s) \| \tilde{\pi}(\cdot \mid s) \big)
$$

**Theorem 1.** 令$$\alpha = D_{\text{TV}}^{\max}(\pi_{\text{old}}, \pi_{\text{new}})$$，且令$$\epsilon = \max_s \vert \mathbb{E}_{a \sim \pi^{\prime}(a \mid s)} [A_{\pi}(s, a)] \vert$$，那么(8)成立。

&emsp;&emsp;由于$$D_{\text{TV}} (p \| q)^2 \le D_{\text{KL}} (p \| q)$$，令$$  D_{\text{KL}}^{\max}(\pi, \tilde{\pi}) = \max_s D_{\text{KL}} \big( \pi(\cdot \mid s) \| \tilde{\pi}(\cdot \mid s) \big)$$，那么由(8)可得

$$\color{green}{\boxed{
    \eta(\tilde{\pi}) 
\le L_\pi(\tilde{\pi}) + C D_{\mathrm{KL}}^{\max }(\pi, \tilde{\pi}), 
\text { where } C = \frac{2 \epsilon \gamma}{(1-\gamma)^2}
}}\tag{10}$$

<figure>
   <img src="assets/img/TRPO_1.JPG" width=400px>
</figure>

<!-- img: "assets/img/portfolio/ninja.png"

![image]({{ page.img | relative_url }}) -->

注意，从这里开始，我们假设adavantage values $$A_{\pi}$$的精确评估。

&emsp;&emsp;下面提出的TRPO将是Alg 1的近似，TRPO通过对KL散度的约束而非惩罚来稳健地允许大的更新。

### 2.3 Optimization of Parameterized Policies

&emsp;&emsp;前面考虑的都是与$$\pi$$的参数化无关的策略优化问题，并且基于所有状态下的策略都可以被评估的假设。下面将描述如何得到根据以上理论基础得到一个实用的算法，在有限采样次数和任意参数化下都实用。

&emsp;&emsp;考虑参数化的策略$$\pi_{\theta}(a \mid s)$$，其参数为$$\theta$$，所以用$$\theta$$来重载之前$$\pi$$的函数。用$$\theta_{\text{old}}$$来表示我们希望改进的旧策略的参数。  
&emsp;&emsp;前面推导出$$\eta(\theta) \le L_{\theta_{\text{old}}} + C D_{\mathrm{KL}}^{\max }(\theta_{\text{old}}, \theta)$$，且在$$\theta = \theta_{\text{old}}$$时取等号。因此通过求以下最小值，可以保证改进真是目标$$\eta$$。

$$ \min_{\theta} [L_{\theta_{\text{old}}} + C D_{\mathrm{KL}}^{\max }(\theta_{\text{old}}, \theta)]$$

&emsp;&emsp;在实际操作中，如果用罚系数$$C$$，步长会非常小。一个比较稳健的采用更大的步长的方法是在新策略和旧策略间的KL散度上加一个约束，i.e. a trust region constraint:

$$\begin{aligned}
    &\min_{\theta} L_{\theta_{\mathrm{old}}}(\theta) \\
    &\text { s.t. } D_{\mathrm{KL}}^{\max }\left(\theta_{\mathrm{old}}, \theta\right) \leq \delta
\end{aligned}$$

但是，因为约束的量太多（状态空间的每个点上的KL散度都受限）所以这个问题不好解。可以用考虑了平均KL散度的启发式近似方法：

$$  \bar{D}_{\mathrm{KL}}^\rho\left(\theta_1, \theta_2\right)
:=  \mathbb{E}_{s \sim \rho} 
    \bigg[  D_{\mathrm{KL}}\big(\pi_{\theta_1}(\cdot \mid s) \| \pi_{\theta_2}(\cdot \mid s)\big)
    \bigg]
$$

因此，提出通过解以下优化问题来生成策略更新：

$$\color{green}{\boxed{\begin{aligned}
    &\min_{\theta}  L_{\theta_{\mathrm{old}}}(\theta) \\
    &\text { s.t. } \bar{D}_{\mathrm{KL}}^{\max }\left(\theta_{\mathrm{old}}, \theta \right) \leq \delta
\end{aligned}}}$$

### 2.4 Sample-Based Estimation of the Objective and Constraint






<!-- 蓝 -->
<b><font color="#3399ff"></font></b>
<!-- 绿 --><!-- #33cc00 -->
<b><font color="#00B050"></font></b>
<!-- 橙 -->
<b><font color="#FF4500"></font></b>